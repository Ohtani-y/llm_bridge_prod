# multinode_sft_ppo.sh ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ©Ÿèƒ½èª¬æ˜

## æ¦‚è¦
`multinode_sft_ppo.sh` ã¯ã€`README_multi_node_SFT_PPO.md` ã®ãƒãƒ«ãƒãƒãƒ¼ãƒ‰åˆ†æ•£å­¦ç¿’æ‰‹é †ã‚’è‡ªå‹•åŒ–ã—ãŸbashã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

## ä¸»ãªç‰¹å¾´

### 1. åˆ†æ•£å­¦ç¿’å¯¾å¿œ
- **SFT**: ãƒãƒ«ãƒãƒãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- **PPO**: ãƒãƒ«ãƒãƒãƒ¼ãƒ‰å¼·åŒ–å­¦ç¿’
- **SLURM**: ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é€£æº
- **Ray Cluster**: åˆ†æ•£å‡¦ç†åŸºç›¤

### 2. ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†
3ã¤ã®ä¸»è¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:
- `sft`: SFTã®ã¿å®Ÿè¡Œ
- `ppo`: PPOã®ã¿å®Ÿè¡Œ  
- `all`: SFTâ†’PPOé †æ¬¡å®Ÿè¡Œ

### 3. æ‰‹å‹•ä»‹å…¥ãƒã‚¤ãƒ³ãƒˆã®æ˜ç¢ºåŒ–
åˆ†æ•£å­¦ç¿’ç‰¹æœ‰ã®æ‰‹å‹•æ“ä½œã‚’æ˜ç¤º:
- SBATCHè¨­å®šã®æ›´æ–°
- Ray clusterã®IPã‚¢ãƒ‰ãƒ¬ã‚¹è¨­å®š
- ãƒãƒ¼ãƒ‰é–“SSHæ¥ç¶š

## è©³ç´°æ©Ÿèƒ½

### **Step 2: ãƒãƒ«ãƒãƒãƒ¼ãƒ‰SFT**

#### Step 2-0: ç’°å¢ƒèµ·å‹• (`step_2_0_python_env_activation`)
```bash
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç’°å¢ƒè¨­å®š
module reset
module load nccl/2.22.3
module load hpcx/2.18.1-gcc-cuda12/hpcx-mt
module load miniconda/24.7.1-py311

# condaç’°å¢ƒæ´»æ€§åŒ–
export CONDA_PATH="~/conda_env"
conda activate $CONDA_PATH
```

#### Step 2-1: ãƒ‡ãƒ¼ã‚¿ç¢ºèª (`step_2_1_download_data_model`)
- Llama-3.2-1B-Instructãƒ¢ãƒ‡ãƒ«å­˜åœ¨ç¢ºèª
- GSM8Kãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå­˜åœ¨ç¢ºèª
- è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã‚ˆã‚‹æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æŒ‡ç¤º

#### Step 2-2: SFTå®Ÿè¡Œ (`step_2_2_multinode_sft_setup`, `step_2_2_submit_sft_job`)
```bash
# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ
~/training/multinode/sft/
â”œâ”€â”€ logs/           # SLURMãƒ­ã‚°
â””â”€â”€ checkpoints/    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«

# SLURMã‚¸ãƒ§ãƒ–æŠ•å…¥
sbatch ~/llm_bridge_prod/train/scripts/mutinode_sft/_sft_llama.sh
```

### **Step 3: ãƒãƒ«ãƒãƒãƒ¼ãƒ‰PPO**

#### Step 3-0: Ray Cluster (`step_3_0_ray_cluster_setup`, `step_3_0_start_ray_cluster`)
```bash
# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ
~/training/multinode/ppo/
â”œâ”€â”€ ray_cluster/logs/  # Ray clusterãƒ­ã‚°
â””â”€â”€ checkpoints/       # PPOãƒ¢ãƒ‡ãƒ«

# Ray clusterèµ·å‹•
sbatch ~/llm_bridge_prod/train/scripts/mutinode_ppo/ray_cluster.sh
```

#### Step 3-1: PPOå®Ÿè¡Œ (`step_3_1_ppo_job_setup`, `step_3_1_run_ppo_training`)
æ‰‹å‹•æ“ä½œãŒå¿…è¦:
```bash
# ãƒ˜ãƒƒãƒ‰ãƒãƒ¼ãƒ‰ã¸SSHæ¥ç¶š
ssh osk-gpu94

# RayçŠ¶æ…‹ç¢ºèª
ray status

# PPOã‚¸ãƒ§ãƒ–æŠ•å…¥
bash ~/llm_bridge_prod/train/scripts/mutinode_ppo/job_submit.sh
```

#### Step 3-2: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (`step_3_2_stop_ray_cluster`)
```bash
# Ray clusteråœæ­¢
ray stop --force
pkill -f ray
scancel <JOB_ID>
```

## ä½¿ç”¨æ–¹æ³•

### **åŸºæœ¬ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**
```bash
# å…¨ä½“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
./multinode_sft_ppo.sh all

# SFTã®ã¿
./multinode_sft_ppo.sh sft

# PPOã®ã¿  
./multinode_sft_ppo.sh ppo
```

### **å€‹åˆ¥ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ**
```bash
# ç’°å¢ƒæº–å‚™
./multinode_sft_ppo.sh step_2_0

# SFTã‚¸ãƒ§ãƒ–æŠ•å…¥
./multinode_sft_ppo.sh submit_sft

# Ray clusterèµ·å‹•
./multinode_sft_ppo.sh start_ray

# PPOè¨­å®š
./multinode_sft_ppo.sh setup_ppo
```

## æ‰‹å‹•è¨­å®šãŒå¿…è¦ãªé …ç›®

### **1. SBATCHè¨­å®š**
ãƒ•ã‚¡ã‚¤ãƒ«: `scripts/mutinode_sft/_sft_llama.sh`, `scripts/mutinode_ppo/ray_cluster.sh`
```bash
#SBATCH -p YOU_TEAM              # â†’ å®Ÿéš›ã®partitionå
#SBATCH --nodelist=osk-gpu[94-95] # â†’ ä½¿ç”¨ãƒãƒ¼ãƒ‰
#SBATCH --nodes=2                 # â†’ ãƒãƒ¼ãƒ‰æ•°
```

### **2. WANDBè¨­å®š**
ãƒ•ã‚¡ã‚¤ãƒ«: `scripts/mutinode_sft/sft_llama.sh`, `scripts/mutinode_ppo/launch_training.py`
```bash
WANDB_ENTITY="YOU_TEAM_ENTITY_NAME"  # â†’ å®Ÿéš›ã®çµ„ç¹”å
```

### **3. Ray Cluster IP**
ãƒ•ã‚¡ã‚¤ãƒ«: `scripts/mutinode_ppo/job_submit.sh`
```bash
HEAD_IP="192.168.11.94:37173"  # â†’ Ray clusterãƒ­ã‚°ã‹ã‚‰å–å¾—
```

## READMEã¨ã®ç›¸é•ç‚¹

### âœ… å®Œå…¨è‡ªå‹•åŒ–ã•ã‚ŒãŸéƒ¨åˆ†
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
- ç’°å¢ƒå¤‰æ•°è¨­å®š
- condaç’°å¢ƒèµ·å‹•
- SLURMã‚¸ãƒ§ãƒ–æŠ•å…¥

### âš ï¸ æ‰‹å‹•æ“ä½œãŒæ®‹ã‚‹éƒ¨åˆ†
- SBATCHè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç·¨é›†
- Ray clusterã®IPã‚¢ãƒ‰ãƒ¬ã‚¹è¨­å®š
- ãƒ˜ãƒƒãƒ‰ãƒãƒ¼ãƒ‰ã§ã®SSHæ“ä½œ
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¤‰æ› (Step 3-3)
- ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Step 3-4)

### ğŸš€ æ”¹å–„ç‚¹
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–
- ãƒ­ã‚°æ©Ÿèƒ½è¿½åŠ 
- ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
- åˆ†ã‹ã‚Šã‚„ã™ã„æ‰‹å‹•æ“ä½œæŒ‡ç¤º

## æ³¨æ„äº‹é …

### **å®Ÿè¡Œç’°å¢ƒ**
- ãƒ­ã‚°ã‚¤ãƒ³ãƒãƒ¼ãƒ‰ã§ã®å®Ÿè¡Œæ¨å¥¨
- è¨ˆç®—ãƒãƒ¼ãƒ‰ã¯SLURMã§è‡ªå‹•å‰²ã‚Šå½“ã¦
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¤‰æ›ã¯è¨ˆç®—ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œ

### **ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†**
- Ray clusterã®é©åˆ‡ãªåœæ­¢ãŒå¿…é ˆ
- æœªåœæ­¢ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¯ãƒªã‚½ãƒ¼ã‚¹å æœ‰ç¶™ç¶š
- SLURMã‚¸ãƒ§ãƒ–IDã®ç®¡ç†ãŒé‡è¦

### **ãƒ‡ãƒ¼ã‚¿æº–å‚™**
- ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¿…é ˆ
- ãƒ‘ã‚¹è¨­å®šã®ç¢ºèªãŒé‡è¦

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### **ã‚ˆãã‚ã‚‹å•é¡Œ**
1. **Ray clusteræ¥ç¶šå¤±æ•—**: IPã‚¢ãƒ‰ãƒ¬ã‚¹è¨­å®šã‚’ç¢ºèª
2. **SLURMæ¨©é™ã‚¨ãƒ©ãƒ¼**: partitionè¨­å®šã‚’ç¢ºèª  
3. **ãƒ¢ãƒ‡ãƒ«æœªç™ºè¦‹**: ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹è¨­å®šã‚’ç¢ºèª
4. **condaç’°å¢ƒã‚¨ãƒ©ãƒ¼**: äº‹å‰ã«`install_conda_env.sh`å®Ÿè¡Œ

### **ãƒ­ã‚°ç¢ºèªæ–¹æ³•**
```bash
# SFTãƒ­ã‚°
tail -f ~/training/multinode/sft/logs/training_sft-*.out

# Ray clusterãƒ­ã‚°  
cat ~/training/multinode/ppo/ray_cluster/logs/slurm-*.out

# PPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ­ã‚°
ray job logs --follow raysubmit_XXX
```