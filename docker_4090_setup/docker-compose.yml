version: '3.8'

services:
  # 単一ノード4GPU構成 (RTX 4090 x4)
  llm-trainer:
    build: .
    container_name: llm-trainer
    hostname: llm-trainer
    privileged: true
    shm_size: 32gb
    environment:
      - NVIDIA_VISIBLE_DEVICES=0,1,2,3
      - CUDA_VISIBLE_DEVICES=0,1,2,3
      - MASTER_ADDR=localhost
      - MASTER_PORT=29500
      - NODE_RANK=0
      - NNODES=1
      - GPUS_PER_NODE=4
      - NCCL_SOCKET_IFNAME=lo
      - NCCL_DEBUG=INFO
      - NCCL_P2P_DISABLE=0
      - NCCL_IB_DISABLE=1
      - WANDB_ENTITY=${WANDB_ENTITY:-YOUR_TEAM_ENTITY_NAME}
      - WANDB_PROJECT_NAME=${WANDB_PROJECT_NAME:-competition_verl_test}
      - WANDB_RUN_NAME=${WANDB_RUN_NAME:-llama3.2_4090_training}
      - WANDB_API_KEY=${WANDB_API_KEY}
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
    volumes:
      - ../:/workspace/llm_bridge_prod
      - ./data:/workspace/data
      - ./models:/workspace/models
      - ./checkpoints:/workspace/checkpoints
      - ./logs:/workspace/logs
    ports:
      - "2222:22"
      - "8265:8265"  # Ray Dashboard
      - "29500:29500"  # PyTorch Distributed Port
      - "6006:6006"   # TensorBoard
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1', '2', '3']
              capabilities: [gpu]
    command: >
      bash -c "
        echo '単一ノード4GPU学習環境起動中...' &&
        echo 'GPU情報確認:' &&
        nvidia-smi &&
        cd /workspace/llm_bridge_prod &&
        /bin/bash
      "
