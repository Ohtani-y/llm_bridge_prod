#!/bin/bash


set -e

echo "ğŸ§ª RTX 4090 x4 ç’°å¢ƒãƒ†ã‚¹ãƒˆé–‹å§‹"
echo "============================="

echo "ğŸš€ ãƒ†ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•ä¸­..."
docker-compose up -d

sleep 30

echo "ğŸ® GPUèªè­˜ãƒ†ã‚¹ãƒˆ..."
docker-compose exec llm-trainer bash -c "conda activate llm_env && python -c '
import torch
print(f\"PyTorch: {torch.__version__}\")
print(f\"CUDA available: {torch.cuda.is_available()}\")
print(f\"GPU count: {torch.cuda.device_count()}\")
for i in range(torch.cuda.device_count()):
    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")
    print(f\"GPU {i} Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB\")
'"

echo "ğŸ“š ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ..."
docker-compose exec llm-trainer bash -c "conda activate llm_env && python -c '
import importlib

modules = [
    \"torch\",
    \"transformers\",
    \"datasets\",
    \"accelerate\",
    \"peft\",
    \"trl\",
    \"wandb\",
    \"flash_attn\",
    \"apex\",
    \"verl.trainer\",
    \"ray\",
    \"transformer_engine\"
]

for mod in modules:
    try:
        importlib.import_module(mod)
        print(f\"âœ… {mod}\")
    except ImportError as e:
        print(f\"âŒ {mod}: {e}\")
'"

echo "ğŸ”— NCCLé€šä¿¡ãƒ†ã‚¹ãƒˆ..."
docker-compose exec llm-trainer bash -c "conda activate llm_env && python -c '
import torch
import torch.distributed as dist
import os

if torch.cuda.is_available():
    device = torch.device(\"cuda:0\")
    tensor = torch.ones(2, 2).to(device)
    print(f\"âœ… CUDA tensor created: {tensor}\")
    print(f\"âœ… GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")
else:
    print(\"âŒ CUDA not available\")
'"

echo "ğŸ¯ å°è¦æ¨¡å­¦ç¿’ãƒ†ã‚¹ãƒˆ..."
docker-compose exec llm-trainer bash -c "conda activate llm_env && python -c '
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM

print(\"å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆé–‹å§‹...\")

model_name = \"gpt2\"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

if torch.cuda.is_available():
    model = model.cuda()
    print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•\")

inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\", padding=True)
if torch.cuda.is_available():
    inputs = {k: v.cuda() for k, v in inputs.items()}

with torch.no_grad():
    outputs = model(**inputs)
    print(f\"âœ… æ¨è«–æˆåŠŸ: output shape {outputs.logits.shape}\")

print(\"âœ… å°è¦æ¨¡å­¦ç¿’ãƒ†ã‚¹ãƒˆå®Œäº†\")
'"

echo ""
echo "ğŸ‰ ç’°å¢ƒãƒ†ã‚¹ãƒˆå®Œäº†ï¼"
echo ""
echo "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:"
echo "1. .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦APIã‚­ãƒ¼ã‚’è¨­å®š"
echo "2. ./scripts/start_training.sh ã§æœ¬æ ¼çš„ãªå­¦ç¿’ã‚’é–‹å§‹"

docker-compose down
