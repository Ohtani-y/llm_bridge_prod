#!/bin/bash


set -e

echo "=== RTX 4090 x4 LLMè‡ªå‹•å­¦ç¿’é–‹å§‹ ==="

if [ -f .env ]; then
    export $(cat .env | grep -v '^#' | xargs)
fi

if [ -z "$WANDB_API_KEY" ] || [ -z "$HUGGINGFACE_API_KEY" ]; then
    echo "âŒ å¿…è¦ãªAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    exit 1
fi

echo "ğŸš€ Dockerã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•ä¸­..."
docker-compose up -d

echo "â³ ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•ã‚’å¾…æ©Ÿä¸­..."
sleep 30

echo "ğŸ” ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã®æº–å‚™ç¢ºèªä¸­..."
docker-compose exec llm-master bash -c "conda activate llm_env && python -c 'import torch; print(f\"PyTorch: {torch.__version__}\"); print(f\"CUDA available: {torch.cuda.is_available()}\"); print(f\"GPU count: {torch.cuda.device_count()}\")'"

echo "ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."
docker-compose exec llm-master bash -c "
    conda activate llm_env &&
    cd /workspace &&
    mkdir -p data/gsm8k &&
    python -c \"
from datasets import load_dataset
import pandas as pd

print('GSM8Kãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...')
train_dataset = load_dataset('gsm8k', 'main', split='train')
test_dataset = load_dataset('gsm8k', 'main', split='test')

train_df = pd.DataFrame(train_dataset)
test_df = pd.DataFrame(test_dataset)

train_df.to_parquet('/workspace/data/gsm8k/train.parquet')
test_df.to_parquet('/workspace/data/gsm8k/test.parquet')

print('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜å®Œäº†')
print(f'å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_df)}ä»¶')
print(f'ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df)}ä»¶')
\"
"

echo "ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."
docker-compose exec llm-master bash -c "
    conda activate llm_env &&
    cd /workspace &&
    mkdir -p models &&
    python -c \"
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

model_name = 'meta-llama/Llama-3.2-1B-Instruct'
save_path = '/workspace/models/Llama-3.2-1B-Instruct'

print(f'ãƒ¢ãƒ‡ãƒ« {model_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...')

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

tokenizer.save_pretrained(save_path)
model.save_pretrained(save_path)

print(f'ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {save_path}')
\"
"

echo "ğŸ¯ SFTå­¦ç¿’é–‹å§‹..."
docker-compose exec llm-master bash -c "
    conda activate llm_env &&
    cd /workspace/llm_bridge_prod &&
    export MASTER_ADDR=llm-master &&
    export MASTER_PORT=37171 &&
    export NODE_RANK=0 &&
    export NNODES=2 &&
    export GPUS_PER_NODE=2 &&
    export NCCL_SOCKET_IFNAME=eth0 &&
    export WANDB_ENTITY=${WANDB_ENTITY} &&
    export WANDB_PROJECT_NAME=${WANDB_PROJECT_NAME} &&
    export WANDB_RUN_NAME=${WANDB_RUN_NAME}_sft &&
    
    torchrun --rdzv_backend c10d \
             --rdzv_endpoint \${MASTER_ADDR}:\${MASTER_PORT} \
             --nnodes \${NNODES} --nproc_per_node \${GPUS_PER_NODE} \
             --node_rank \${NODE_RANK} \
             -m verl.trainer.fsdp_sft_trainer \
             data.train_files=/workspace/data/gsm8k/train.parquet \
             data.val_files=/workspace/data/gsm8k/test.parquet \
             data.prompt_key=question \
             data.response_key=answer \
             data.micro_batch_size_per_gpu=4 \
             model.partial_pretrain=/workspace/models/Llama-3.2-1B-Instruct \
             trainer.project_name=gsm8k-sft-4090 \
             trainer.experiment_name=/workspace/checkpoints/sft_llama_4090 \
             trainer.total_epochs=2 \
             trainer.save_freq=500 \
             trainer.eval_freq=100 \
             trainer.log_freq=10
" &

echo "ğŸ”— ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰å­¦ç¿’å‚åŠ ..."
sleep 10
docker-compose exec llm-worker bash -c "
    conda activate llm_env &&
    cd /workspace/llm_bridge_prod &&
    export MASTER_ADDR=llm-master &&
    export MASTER_PORT=37171 &&
    export NODE_RANK=1 &&
    export NNODES=2 &&
    export GPUS_PER_NODE=2 &&
    export NCCL_SOCKET_IFNAME=eth0 &&
    export WANDB_ENTITY=${WANDB_ENTITY} &&
    export WANDB_PROJECT_NAME=${WANDB_PROJECT_NAME} &&
    export WANDB_RUN_NAME=${WANDB_RUN_NAME}_sft &&
    
    torchrun --rdzv_backend c10d \
             --rdzv_endpoint \${MASTER_ADDR}:\${MASTER_PORT} \
             --nnodes \${NNODES} --nproc_per_node \${GPUS_PER_NODE} \
             --node_rank \${NODE_RANK} \
             -m verl.trainer.fsdp_sft_trainer \
             data.train_files=/workspace/data/gsm8k/train.parquet \
             data.val_files=/workspace/data/gsm8k/test.parquet \
             data.prompt_key=question \
             data.response_key=answer \
             data.micro_batch_size_per_gpu=4 \
             model.partial_pretrain=/workspace/models/Llama-3.2-1B-Instruct \
             trainer.project_name=gsm8k-sft-4090 \
             trainer.experiment_name=/workspace/checkpoints/sft_llama_4090 \
             trainer.total_epochs=2 \
             trainer.save_freq=500 \
             trainer.eval_freq=100 \
             trainer.log_freq=10
" &

echo "ğŸ‰ å­¦ç¿’é–‹å§‹å®Œäº†ï¼"
echo ""
echo "ğŸ“Š å­¦ç¿’çŠ¶æ³ã®ç¢ºèªæ–¹æ³•:"
echo "1. WandB: https://wandb.ai/${WANDB_ENTITY}/${WANDB_PROJECT_NAME}"
echo "2. ãƒ­ã‚°ç¢ºèª: docker-compose logs -f llm-master"
echo "3. GPUä½¿ç”¨ç‡: docker-compose exec llm-master nvidia-smi"
echo ""
echo "â¹ï¸  å­¦ç¿’åœæ­¢: ./scripts/stop_training.sh"

wait
