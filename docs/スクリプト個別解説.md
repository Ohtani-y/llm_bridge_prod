# スクリプト個別解説 - LLM Bridge Prod

このドキュメントでは、`llm_bridge_prod`リポジトリに含まれる各スクリプトとPythonコードの詳細な解説を提供します。

## 目次

1. [Pythonスクリプト](#pythonスクリプト)
   - [upload_tokenizer_and_finetuned_model_to_huggingface_hub.py](#1-upload_tokenizer_and_finetuned_model_to_huggingface_hubpy)
   - [launch_training.py](#2-launch_trainingpy)

2. [シェルスクリプト](#シェルスクリプト)
   - [sft_llama.sh](#3-sft_llamash)
   - [_sft_llama.sh](#4-_sft_llamash)
   - [ray_cluster.sh](#5-ray_clustersh)
   - [job_submit.sh](#6-job_submitsh)

---

## Pythonスクリプト

### 1. upload_tokenizer_and_finetuned_model_to_huggingface_hub.py

**場所**: `train/scripts/upload_tokenizer_and_finetuned_model_to_huggingface_hub.py`

#### 概要
学習済みモデルとトークナイザーをHugging Face Hubにアップロードするためのユーティリティスクリプトです。

#### 主な機能
- ローカルのモデルディレクトリをHugging Face Hubにアップロード
- 大容量ファイルの自動チャンク化と並列アップロード
- リポジトリの自動作成（存在しない場合）
- 一時ファイルの自動除外

#### 処理手順
1. **引数解析**: コマンドライン引数から必要な情報を取得
   - `--input_tokenizer_and_model_dir`: アップロードするローカルディレクトリ
   - `--hf_token`: Hugging Face アクセストークン
   - `--repo_id`: アップロード先リポジトリID（`owner/repo_name`形式）

2. **認証**: Hugging Face APIに認証
   ```python
   login(token=token)
   ```

3. **リポジトリ作成**: 指定されたリポジトリが存在しない場合は自動作成
   ```python
   api.create_repo(repo_id, repo_type="model", exist_ok=True)
   ```

4. **ファイルアップロード**: フォルダ全体をアップロード
   ```python
   upload_folder(
       repo_id=repo_id,
       folder_path=local_dir,
       repo_type="model",
       path_in_repo="",
       ignore_patterns=["*.tmp"]
   )
   ```

#### 使用例
```bash
python upload_tokenizer_and_finetuned_model_to_huggingface_hub.py \
    --input_tokenizer_and_model_dir ~/model/fine_tuned_llama \
    --hf_token hf_xxxxxxxxxxxx \
    --repo_id your_team/competition_model
```

#### 依存関係
- `huggingface_hub`: Hugging Face APIとの連携
- `argparse`: コマンドライン引数の解析

---

### 2. launch_training.py

**場所**: `train/scripts/mutinode_ppo/launch_training.py`

#### 概要
マルチノードでのPPO（Proximal Policy Optimization）強化学習を実行するためのランチャースクリプトです。

#### 主な機能
- Verl フレームワークを使用したPPO学習の設定
- マルチノード分散学習の設定
- WandBログ設定
- GSM8Kデータセットを使用した数学問題解決の学習

#### 処理手順
1. **環境設定**: 分散学習用の環境変数設定
   ```python
   NNODES = 2
   GPUS_PER_NODE = 8
   ```

2. **WandB設定**: 実験管理用の設定
   ```python
   WANDB_ENTITY = "YOU_TEAM_ENTITY_NAME"
   WANDB_PROJECT_NAME = "competition_verl_test"
   WANDB_RUN_NAME = "llama3.2_SFT_multinode_ppo"
   ```

3. **学習パラメータ設定**: PPO学習用の詳細パラメータ
   - **データ設定**:
     - `data.train_files`: 学習データ（GSM8K train.parquet）
     - `data.val_files`: 検証データ（GSM8K test.parquet）
     - `data.train_batch_size=256`: 学習バッチサイズ
     - `data.max_prompt_length=512`: 最大プロンプト長
     - `data.max_response_length=256`: 最大応答長

   - **Actor設定**:
     - `actor_rollout_ref.model.path`: ベースモデル（Llama-3.2-1B-Instruct）
     - `actor_rollout_ref.actor.optim.lr=1e-6`: Actor学習率
     - `actor_rollout_ref.actor.ppo_mini_batch_size=64`: PPOミニバッチサイズ

   - **Critic設定**:
     - `critic.model.path`: Criticモデルパス
     - `critic.optim.lr=1e-5`: Critic学習率

   - **アルゴリズム設定**:
     - `algorithm.kl_ctrl.kl_coef=0.001`: KLダイバージェンス係数

4. **学習実行**: Verl PPOトレーナーの起動
   ```python
   from verl.trainer import main_ppo
   sys.argv = ["verl.trainer.main_ppo"] + args
   main_ppo.main()
   ```

#### 技術的詳細
- **PPO**: 強化学習アルゴリズムで、人間のフィードバックを模倣した報酬システムでモデルを改善
- **Actor-Critic**: Actorが行動を決定し、Criticが行動の価値を評価する構造
- **KL制御**: モデルが元の行動から大きく逸脱しないよう制御

#### 依存関係
- `verl.trainer`: PPO学習フレームワーク
- `os`, `sys`: システム操作

---

## シェルスクリプト

### 3. sft_llama.sh

**場所**: `train/scripts/mutinode_sft/sft_llama.sh`

#### 概要
シングルノードでのSFT（Supervised Fine-Tuning）を実行するためのスクリプトです。

#### 主な機能
- 環境モジュールの読み込み
- 分散学習用環境変数の設定
- TorchRunを使用したマルチGPU学習
- GSM8Kデータセットでのファインチューニング

#### 処理手順
1. **環境セットアップ**:
   ```bash
   source /etc/profile.d/modules.sh
   module reset
   module load hpcx/2.18.1-gcc-cuda12/hpcx-mt
   module load miniconda/24.7.1-py311
   ```

2. **Conda環境の初期化**:
   ```bash
   conda init
   conda config --set auto_activate_base false
   source ~/.bashrc
   conda activate $CONDA_PATH
   ```

3. **分散学習パラメータの設定**:
   - `MASTER_ADDR`: マスターノードのIPアドレス
   - `MASTER_PORT`: 通信ポート
   - `NODE_RANK`: ノードランク
   - `NNODES`: 総ノード数
   - `GPUS_PER_NODE`: ノードあたりのGPU数

4. **環境変数の設定**:
   ```bash
   export NCCL_SOCKET_IFNAME=enp25s0np0
   export NVTE_FUSED_ATTN=0
   export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
   ```

5. **WandB設定**:
   ```bash
   export WANDB_ENTITY="YOU_TEAM_ENTITY_NAME"
   export WANDB_PROJECT_NAME="competition_verl_test"
   export WANDB_RUN_NAME="llama3.2_SFT_test"
   ```

6. **TorchRun実行**:
   ```bash
   torchrun --rdzv_backend c10d \
            --rdzv_endpoint ${MASTER_ADDR}:${MASTER_PORT} \
            --nnodes ${NNODES} --nproc_per_node ${GPUS_PER_NODE} \
            --node_rank ${NODE_RANK} \
            -m verl.trainer.fsdp_sft_trainer \
            [学習パラメータ...]
   ```

#### 学習パラメータ詳細
- `data.train_files`: GSM8K学習データ
- `data.val_files`: GSM8K検証データ
- `data.prompt_key=extra_info`: プロンプトキー
- `data.response_key=extra_info`: 応答キー
- `data.prompt_dict_keys=['question']`: 質問フィールド
- `data.response_dict_keys=['answer']`: 回答フィールド
- `data.micro_batch_size_per_gpu=8`: GPU毎のマイクロバッチサイズ
- `model.partial_pretrain`: 事前学習モデルパス
- `trainer.total_epochs=2`: 学習エポック数

#### 技術的詳細
- **FSDP**: Fully Sharded Data Parallel - メモリ効率的な分散学習
- **TorchRun**: PyTorchの分散学習ランチャー
- **NCCL**: GPU間通信ライブラリ

---

### 4. _sft_llama.sh

**場所**: `train/scripts/mutinode_sft/_sft_llama.sh`

#### 概要
マルチノードSFT学習のオーケストレーションを行うSLURMジョブスクリプトです。

#### 主な機能
- SLURM環境での複数ノード管理
- ノードリストの動的解析
- SSH経由での各ノードでの学習実行
- 並列実行とログ管理

#### 処理手順
1. **SLURMジョブ設定**:
   ```bash
   #SBATCH --job-name=training_sft
   #SBATCH -p YOU_TEAM_ENTITY_NAME
   #SBATCH --nodelist=osk-gpu[YOU_TEAM_GPU_NUM]
   #SBATCH --nodes=2
   #SBATCH --gpus-per-node=8
   ```

2. **マスターノード情報の取得**:
   ```bash
   MASTER_ADDR=$(scontrol show hostname $SLURM_JOB_NODELIST | head -n1)
   MASTER_PORT=37171
   NNODES=$SLURM_JOB_NUM_NODES
   GPUS_PER_NODE=$SLURM_GPUS_PER_NODE
   ```

3. **ノードリストの解析**:
   ```bash
   # "isk-gpu[02-03]" 形式からノード名を抽出
   prefix=$(echo $input_nodes | grep -o "^[^[]*")
   range=$(echo $input_nodes | grep -oP "\[(\K[^\]]+)")
   ```

4. **ノード配列の構築**:
   ```bash
   # 範囲指定（例：02-03）を個別ノード名に展開
   for (( i=10#$start; i<=10#$end; i++ )); do
       NODES+=("$prefix$(printf "%02d" "$i")")
   done
   ```

5. **各ノードでの学習実行**:
   ```bash
   for node in "${NODES[@]}"; do
       ssh -q $node "
           cd $SCRIPT_ROOT && \
           bash $SCRIPT_ROOT/scripts/mutinode_sft/sft_llama.sh \
               $MASTER_ADDR $MASTER_PORT $NODE_RANK $NNODES $GPUS_PER_NODE
       " &
       ((NODE_RANK+=1))
   done
   ```

6. **並列実行の待機**:
   ```bash
   wait  # 全てのバックグラウンドジョブの完了を待機
   ```

#### 技術的詳細
- **SLURM**: ジョブスケジューラー
- **SSH並列実行**: 各ノードで同時に学習プロセスを起動
- **ノードランク**: 分散学習での各ノードの識別子

---

### 5. ray_cluster.sh

**場所**: `train/scripts/mutinode_ppo/ray_cluster.sh`

#### 概要
Ray分散計算クラスターをSLURMジョブとして起動し、PPO学習環境を構築するスクリプトです。

#### 主な機能
- Rayクラスターの自動セットアップ
- ヘルスチェック機能
- ネットワーク最適化設定
- 長時間実行の監視

#### 処理手順
1. **SLURMジョブ設定**:
   ```bash
   #SBATCH --job-name=verl-ray-ppo
   #SBATCH --nodes=2
   #SBATCH --ntasks-per-node=2
   #SBATCH --gpus-per-node=8
   #SBATCH --cpus-per-task=64
   #SBATCH --time=6-00:00:00
   ```

2. **ネットワーク最適化設定**:
   ```bash
   export NCCL_DEBUG=TRACE
   export GPU_MAX_HW_QUEUES=2
   export TORCH_NCCL_HIGH_PRIORITY=1
   export NCCL_IB_HCA=mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_8,mlx5_9
   export NCCL_IB_GID_INDEX=3
   ```

3. **クラスタートポロジーの計算**:
   ```bash
   nodes_array=($(scontrol show hostnames "$SLURM_JOB_NODELIST" | tr '\n' ' '))
   head_node=${nodes_array[0]}
   port=37173
   head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)
   ```

4. **Rayヘッドノードの起動**:
   ```bash
   srun --nodes=1 --ntasks=1 -w "$head_node" \
     bash -c "ray start --head --node-ip-address=$head_node_ip --port=$port \
              --dashboard-port=$dashboard_port --dashboard-host=0.0.0.0 \
              --num-cpus=$SLURM_CPUS_PER_TASK --num-gpus=$SLURM_GPUS_PER_NODE --block"
   ```

5. **Rayワーカーノードの起動**:
   ```bash
   for ((i = 1; i <= worker_num; i++)); do
     node_i=${nodes_array[$i]}
     srun --nodes=1 --ntasks=1 -w "$node_i" \
       bash -c "ray start --address $ip_head \
                --num-cpus=$SLURM_CPUS_PER_TASK --num-gpus=$SLURM_GPUS_PER_NODE --block"
   done
   ```

6. **接続テスト**:
   ```bash
   srun --overlap -N1 -n1 -c1 --gpus=0 -w "$head_node" \
     bash -c "$CONDA_PATH/bin/python - <<'PY'
   import ray, json
   ray.init(address='$ip_head')
   print(json.dumps({'nodes': len(ray.nodes()), ...}))
   PY"
   ```

7. **ヘルスチェック監視**:
   ```bash
   while true; do
       # プロセス生存確認
       for pid in "${ray_pids[@]}"; do
           if ! kill -0 "$pid" 2>/dev/null; then
               echo "[ERROR] Ray process $pid has exited."
               exit 1
           fi
       done
       
       # ダッシュボードヘルスチェック
       if ! curl -sf --max-time 10 "$ray_health_url" >/dev/null; then
           echo "[ERROR] Ray dashboard health check failed"
           exit 1
       fi
       
       sleep 300  # 5分間隔でチェック
   done
   ```

#### 技術的詳細
- **Ray**: 分散計算フレームワーク
- **InfiniBand**: 高速ネットワーク通信
- **ダッシュボード**: Rayクラスターの監視UI
- **ヘルスチェック**: システムの安定性監視

---

### 6. job_submit.sh

**場所**: `train/scripts/mutinode_ppo/job_submit.sh`

#### 概要
Rayクラスターに対してPPO学習ジョブを投入するためのクライアントスクリプトです。

#### 主な機能
- Ray Jobの投入
- 環境設定の簡素化
- リモート実行の管理

#### 処理手順
1. **環境モジュールの読み込み**:
   ```bash
   source /etc/profile.d/modules.sh
   module reset
   module load hpcx/2.18.1-gcc-cuda12/hpcx-mt
   module load miniconda/24.7.1-py311
   ```

2. **Conda環境の設定**:
   ```bash
   conda init
   conda config --set auto_activate_base false
   source ~/.bashrc
   conda activate $CONDA_PATH
   ```

3. **環境変数の設定**:
   ```bash
   export CONDA_PATH="~/conda_env"
   export NCCL_SOCKET_IFNAME=enp25s0np0
   export NVTE_FUSED_ATTN=0
   export NVTE_DEBUG=1
   export NVTE_DEBUG_LEVEL=0
   ```

4. **Rayジョブの投入**:
   ```bash
   HEAD_IP="192.168.11.94:37173"
   
   ray job submit --address=$HEAD_IP \
       --no-wait \
       -- \
       $CONDA_PATH/bin/python $HOME/llm_bridge_prod/train/scripts/mutinode_ppo/launch_training.py
   ```

#### 技術的詳細
- **Ray Job**: Rayクラスター上でのジョブ実行システム
- `--no-wait`: ジョブ投入後、完了を待たずに終了
- **リモート実行**: 既に起動済みのRayクラスターに対してジョブを投入

---

## スクリプト間の関係性

### SFT（教師ありファインチューニング）ワークフロー
1. `_sft_llama.sh` → 複数ノードでの学習オーケストレーション
2. `sft_llama.sh` → 各ノードでの実際の学習実行

### PPO（強化学習）ワークフロー
1. `ray_cluster.sh` → Rayクラスターの起動と監視
2. `job_submit.sh` → PPO学習ジョブの投入
3. `launch_training.py` → 実際のPPO学習の実行

### モデル管理ワークフロー
1. SFT/PPO学習完了後
2. `upload_tokenizer_and_finetuned_model_to_huggingface_hub.py` → モデルのアップロード

## 実行順序の例

### シングルノードSFT
```bash
# 直接実行
bash train/scripts/mutinode_sft/sft_llama.sh [MASTER_ADDR] [MASTER_PORT] [NODE_RANK] [NNODES] [GPUS_PER_NODE]
```

### マルチノードSFT
```bash
# SLURMジョブとして実行
sbatch train/scripts/mutinode_sft/_sft_llama.sh
```

### マルチノードPPO
```bash
# 1. Rayクラスターの起動
sbatch train/scripts/mutinode_ppo/ray_cluster.sh

# 2. PPOジョブの投入
bash train/scripts/mutinode_ppo/job_submit.sh
```

### モデルアップロード
```bash
python train/scripts/upload_tokenizer_and_finetuned_model_to_huggingface_hub.py \
    --input_tokenizer_and_model_dir ~/training/checkpoints/final \
    --hf_token $HF_TOKEN \
    --repo_id your_team/competition_model
```

## トラブルシューティング

### よくある問題と解決策

1. **NCCL通信エラー**
   - `NCCL_SOCKET_IFNAME`の設定を確認
   - ネットワークインターフェースの状態をチェック

2. **Ray接続エラー**
   - ヘッドノードのIPアドレスとポートを確認
   - ファイアウォール設定をチェック

3. **GPU メモリ不足**
   - `micro_batch_size_per_gpu`を小さくする
   - `gpu_memory_utilization`を調整

4. **SLURM ジョブエラー**
   - ノードリストの形式を確認
   - パーティション設定をチェック

このドキュメントにより、各スクリプトの役割と実行方法を理解し、効率的にLLM学習を実行できるようになります。
